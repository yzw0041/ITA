{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nilearn.plotting import plot_epi\n",
    "from nilearn import plotting\n",
    "from nilearn.datasets import load_mni152_template\n",
    "from nilearn.image import resample_to_img\n",
    "from nilearn import input_data\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "import nitime\n",
    "#Import the time-series objects:\n",
    "from nitime.timeseries import TimeSeries\n",
    "#Import the analysis objects:\n",
    "from nitime.analysis import CorrelationAnalyzer, CoherenceAnalyzer\n",
    "#Import utility functions:\n",
    "from nitime.utils import percent_change\n",
    "from nitime.viz import drawmatrix_channels, drawgraph_channels, plot_xcorr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_dir=\"/Volumes/Pegasus_wangyun/ADHD/ITA/MRI/ITA_version1/results/preprocessing\"\n",
    "!find $prep_dir/niftiDATA_*Session*.nii > conn_denoise.csv\n",
    "F=pd.read_csv(\"conn_denoise.csv\",header=None,names={'files'})\n",
    "F.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matlab matrix in conn is too big to load \n",
    "pp='/Volumes/Pegasus_wangyun/ADHD/ITA/MRI/'\n",
    "M=pd.read_csv(f'{pp}conn_sess_cond.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_dir=\"/Volumes/Pegasus_wangyun/ADHD/ADHD_HC_ADD/conn_ADHD_add/results/preprocessing\"\n",
    "!find $prep_dir/niftiDATA_*Session*.nii > hc_conn_denoise.csv\n",
    "F=pd.read_csv(\"hc_conn_denoise.csv\",header=None,names={'files'})\n",
    "F.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matlab matrix in conn is too big to load \n",
    "#pp='/Volumes/Pegasus_wangyun/ADHD/ITA/MRI/'\n",
    "M=pd.read_csv('hc_conn_sess_cond.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matlab matrix in conn is too big to load \n",
    "pp='/Users/posnerlab/Documents/GitHub/CBIG/stable_projects/preprocessing/CBIG_fMRI_Preproc2016/HC_prep/'\n",
    "M=pd.read_csv(f'{pp}hc_conn_sess_cond.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matlab matrix in conn is too big to load \n",
    "pp='/Users/posnerlab/Documents/GitHub/CBIG/stable_projects/preprocessing/CBIG_fMRI_Preproc2016/HC_prep/'\n",
    "M=pd.read_excel(f'{pp}HC_conn_sess_cond.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matlab matrix in conn is too big to load \n",
    "pp='/Users/posnerlab/Documents/GitHub/CBIG/stable_projects/preprocessing/CBIG_fMRI_Preproc2016/HC_prep/'\n",
    "M=pd.read_excel(f'{pp}HC_conn_sess_cond.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "\n",
    "sub=[]\n",
    "sess=[]\n",
    "X1=F[F['files'].str.contains(\"Session1\")].reset_index().drop(columns=['index'])\n",
    "for i,j in enumerate(X1['files']):\n",
    "    sub.append('sub'+X1['files'][i].split('/')[-1].split('_')[1][-2:])\n",
    "    #print(sub)\n",
    "    sess.append(X1['files'][i].split('/')[-1].split('_')[-1].split('.')[0])\n",
    "    #print(sess)\n",
    "X1['ID']=sub\n",
    "\n",
    "sub=[]\n",
    "sess=[]\n",
    "X2=F[F['files'].str.contains(\"Session2\")].reset_index().drop(columns=['index'])\n",
    "for i,j in enumerate(X2['files']):\n",
    "    sub.append('sub'+X2['files'][i].split('/')[-1].split('_')[1][-2:])\n",
    "    #print(sub)\n",
    "    sess.append(X2['files'][i].split('/')[-1].split('_')[-1].split('.')[0])\n",
    "    #print(sess)\n",
    "X2['ID']=sub\n",
    "X1.merge(X2,how='outer',on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "\n",
    "sub=[]\n",
    "sess=[]\n",
    "X1=F[F['files'].str.contains(\"Session1\")].reset_index().drop(columns=['index'])\n",
    "for i,j in enumerate(X1['files']):\n",
    "    sub.append('sub'+X1['files'][i].split('/')[-1].split('_')[1][-2:])\n",
    "    #print(sub)\n",
    "    sess.append(X1['files'][i].split('/')[-1].split('_')[-1].split('.')[0])\n",
    "    #print(sess)\n",
    "X1['ID']=sub\n",
    "\n",
    "sub=[]\n",
    "sess=[]\n",
    "X2=F[F['files'].str.contains(\"Session2\")].reset_index().drop(columns=['index'])\n",
    "for i,j in enumerate(X2['files']):\n",
    "    sub.append('sub'+X2['files'][i].split('/')[-1].split('_')[1][-2:])\n",
    "    #print(sub)\n",
    "    sess.append(X2['files'][i].split('/')[-1].split('_')[-1].split('.')[0])\n",
    "    #print(sess)\n",
    "X2['ID']=sub\n",
    "#X1.merge(X2,how='outer',on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = X1.merge(\n",
    "    X2, how='outer', on='ID').rename(columns={\n",
    "        'files_x': 'session1',\n",
    "        'files_y': 'session2'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF=M.merge(XX,on='ID')\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD=pd.DataFrame(columns=['ID','pairs'])\n",
    "keys=('ID','pairs')\n",
    "for i in np.arange(DF.shape[0]):\n",
    "    ll=[]\n",
    "    if i:\n",
    "        ll.append(DF['session1'][i])\n",
    "    \n",
    "    if ll: \n",
    "        #print(image.concat_imgs(ll).shape)\n",
    "        DD=DD.append(pd.Series(dict(zip(keys,(DF['ID'][i],ll)))),ignore_index=True)\n",
    "DD.rename(columns={'pairs':'post_list'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD=pd.DataFrame(columns=['ID','pairs'])\n",
    "keys=('ID','pairs')\n",
    "for i in np.arange(DF.shape[0]):\n",
    "    ll=[]\n",
    "    if i:\n",
    "        ll.append(DF['session1'][i])\n",
    "    \n",
    "    if ll: \n",
    "        #print(image.concat_imgs(ll).shape)\n",
    "        DD=DD.append(pd.Series(dict(zip(keys,(DF['ID'][i],ll)))),ignore_index=True)\n",
    "DD.rename(columns={'pairs':'list'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD['list'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD=pd.DataFrame(columns=['ID','pairs'])\n",
    "keys=('ID','pairs')\n",
    "for i in np.arange(DF.shape[0]):\n",
    "    ll=[]\n",
    "    if DF['session1'][i]:\n",
    "        ll.append(DF['session1'][i])\n",
    "    if DF['session2'][i]:\n",
    "        ll.append(DF['session2'][i])\n",
    "    \n",
    "    if ll: \n",
    "        #print(image.concat_imgs(ll).shape)\n",
    "        DD=DD.append(pd.Series(dict(zip(keys,(DF['ID'][i],ll)))),ignore_index=True)\n",
    "DD.rename(columns={'pairs':'list'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD['list'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD['list'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF['session2'][i].isnan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF['session2'][i].values.isnan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF['session2'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "print math.isnan(float(DF['session2'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.isnan(float(DF['session2'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF['session2'][i].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(DF['session2'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD=pd.DataFrame(columns=['ID','pairs'])\n",
    "keys=('ID','pairs')\n",
    "for i in np.arange(DF.shape[0]):\n",
    "    ll=[]\n",
    "    if ~pd.isna(DF['session1'][i]):\n",
    "        ll.append(DF['session1'][i])\n",
    "    if ~pd.isna(DF['session2'][i]):\n",
    "        ll.append(DF['session2'][i])\n",
    "    \n",
    "    if ll: \n",
    "        #print(image.concat_imgs(ll).shape)\n",
    "        DD=DD.append(pd.Series(dict(zip(keys,(DF['ID'][i],ll)))),ignore_index=True)\n",
    "DD.rename(columns={'pairs':'list'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD['list'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "All=DDD.merge(DD,on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD['list'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "~pd.isna(DF['session2'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(DF['session2'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pd.isna(DF['session2'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(DF['session2'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "not pd.isna(DF['session2'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD=pd.DataFrame(columns=['ID','pairs'])\n",
    "keys=('ID','pairs')\n",
    "for i in np.arange(DF.shape[0]):\n",
    "    ll=[]\n",
    "    if not pd.isna(DF['session1'][i]):\n",
    "        ll.append(DF['session1'][i])\n",
    "    if not pd.isna(DF['session2'][i]):\n",
    "        ll.append(DF['session2'][i])\n",
    "    \n",
    "    if ll: \n",
    "        #print(image.concat_imgs(ll).shape)\n",
    "        DD=DD.append(pd.Series(dict(zip(keys,(DF['ID'][i],ll)))),ignore_index=True)\n",
    "DD.rename(columns={'pairs':'list'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "DD['list'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC=[]\n",
    "Cohe=[]\n",
    "\n",
    "for i in np.arange(DD.shape[0]):\n",
    "    \n",
    "    # concatenate images for post and pre separately\n",
    "     #ll=[DF['session1'][i],DF['session2'][i]]\n",
    "    #X1 = image.load_img(DF['session1'][i]) # post session1\n",
    "    #X2 = image.load_img(DF['session2'][i]) # post session2\n",
    "   \n",
    "    spheres_masker = input_data.NiftiSpheresMasker(\n",
    "        seeds=coords, smoothing_fwhm=4, radius=5.,detrend=True,\n",
    "     low_pass=0.1, high_pass=0.01, t_r=2.2,memory='nilearn_cache', memory_level=1)\n",
    "    X=image.concat_imgs(DD['list'][i])\n",
    "\n",
    "    print(X.shape)\n",
    "    time_series2 = spheres_masker.fit_transform(X)\n",
    "    correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "    correlation_matrix = correlation_measure.fit_transform([time_series2])[0]\n",
    "    FC.append(correlation_matrix)\n",
    "    data = percent_change(time_series2.T)\n",
    "    T = TimeSeries(data, sampling_interval=DF['TR'][i], time_unit='s')\n",
    "    C =CoherenceAnalyzer(T)\n",
    "    f_lb=0.01\n",
    "    f_ub=0.1\n",
    "    freq_idx = np.where((C.frequencies > f_lb) * (C.frequencies < f_ub))[0]\n",
    "\n",
    "    Cohe.append(np.mean(C.coherence[:, :, freq_idx], -1))  # Averaging on the last dimension\n",
    "    #fig03 = drawmatrix_channels(coh,  size=[10., 10.], color_anchor=0)\n",
    "np.save('power_correlation_HC.npy', FC)\n",
    "np.save('power_coherence_HC.npy', Cohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "power = datasets.fetch_coords_power_2011()\n",
    "print('Power atlas comes with {0}.'.format(power.keys()))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "coords = np.vstack((power.rois['x'], power.rois['y'], power.rois['z'])).T\n",
    "\n",
    "print('Stacked power coordinates in array of shape {0}.'.format(coords.shape))\n",
    "\n",
    "from nilearn import input_data\n",
    "\n",
    "spheres_masker = input_data.NiftiSpheresMasker(\n",
    "    seeds=coords, smoothing_fwhm=4, radius=5.,detrend=True,\n",
    " low_pass=0.1, high_pass=0.01, t_r=2.2,memory='nilearn_cache', memory_level=1)\n",
    "msdl_data = datasets.fetch_atlas_msdl()\n",
    "msdl_coords = msdl_data.region_coords\n",
    "n_regions = len(msdl_coords)\n",
    "masker = input_data.NiftiMapsMasker(msdl_data.maps, resampling_target=\"data\", t_r=2.2, detrend=True,\n",
    "    low_pass=.1, high_pass=.01, memory='nilearn_cache', memory_level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC=[]\n",
    "Cohe=[]\n",
    "\n",
    "for i in np.arange(DD.shape[0]):\n",
    "    \n",
    "    # concatenate images for post and pre separately\n",
    "     #ll=[DF['session1'][i],DF['session2'][i]]\n",
    "    #X1 = image.load_img(DF['session1'][i]) # post session1\n",
    "    #X2 = image.load_img(DF['session2'][i]) # post session2\n",
    "   \n",
    "    spheres_masker = input_data.NiftiSpheresMasker(\n",
    "        seeds=coords, smoothing_fwhm=4, radius=5.,detrend=True,\n",
    "     low_pass=0.1, high_pass=0.01, t_r=2.2,memory='nilearn_cache', memory_level=1)\n",
    "    X=image.concat_imgs(DD['list'][i])\n",
    "\n",
    "    print(X.shape)\n",
    "    time_series2 = spheres_masker.fit_transform(X)\n",
    "    correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "    correlation_matrix = correlation_measure.fit_transform([time_series2])[0]\n",
    "    FC.append(correlation_matrix)\n",
    "    data = percent_change(time_series2.T)\n",
    "    T = TimeSeries(data, sampling_interval=DF['TR'][i], time_unit='s')\n",
    "    C =CoherenceAnalyzer(T)\n",
    "    f_lb=0.01\n",
    "    f_ub=0.1\n",
    "    freq_idx = np.where((C.frequencies > f_lb) * (C.frequencies < f_ub))[0]\n",
    "\n",
    "    Cohe.append(np.mean(C.coherence[:, :, freq_idx], -1))  # Averaging on the last dimension\n",
    "    #fig03 = drawmatrix_channels(coh,  size=[10., 10.], color_anchor=0)\n",
    "np.save('power_correlation_HC.npy', FC)\n",
    "np.save('power_coherence_HC.npy', Cohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(FC[31].ravel(),bins=200)\n",
    "plt.hist(Cohe[31].ravel(),alpha=0.6,bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(FC[31].ravel(),bins=200)\n",
    "plt.hist(Cohe[31].ravel(),alpha=0.6,bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(FC[1].ravel(),bins=200)\n",
    "plt.hist(Cohe[1].ravel(),alpha=0.6,bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "<style>\n",
    "td {\n",
    "  font-size: 50px\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import check_call\n",
    "\n",
    "def post_save(model, os_path, contents_manager):\n",
    "    \"\"\"post-save hook for converting notebooks to .py scripts\"\"\"\n",
    "    if model['type'] != 'notebook':\n",
    "        return # only do this for notebooks\n",
    "    d, fname = os.path.split(os_path)\n",
    "    check_call(['ipython', 'nbconvert', '--to', 'script', fname], cwd=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_save('notebook','/Users/posnerlab/Documents/GitHub/ITA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_save('notebook','/Users/posnerlab/Documents/GitHub/ITA',contents_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "from notebook.utils import to_api_path\n",
    "\n",
    "_script_exporter = None\n",
    "\n",
    "def script_post_save(model, os_path, contents_manager, **kwargs):\n",
    "    \"\"\"convert notebooks to Python script after save with nbconvert\n",
    "\n",
    "    replaces `jupyter notebook --script`\n",
    "    \"\"\"\n",
    "    from nbconvert.exporters.script import ScriptExporter\n",
    "\n",
    "    if model['type'] != 'notebook':\n",
    "        return\n",
    "\n",
    "    global _script_exporter\n",
    "\n",
    "    if _script_exporter is None:\n",
    "        _script_exporter = ScriptExporter(parent=contents_manager)\n",
    "\n",
    "    log = contents_manager.log\n",
    "\n",
    "    base, ext = os.path.splitext(os_path)\n",
    "    script, resources = _script_exporter.from_filename(os_path)\n",
    "    script_fname = base + resources.get('output_extension', '.txt')\n",
    "    log.info(\"Saving script /%s\", to_api_path(script_fname, contents_manager.root_dir))\n",
    "\n",
    "    with io.open(script_fname, 'w', encoding='utf-8') as f:\n",
    "        f.write(script)\n",
    "\n",
    "c.FileContentsManager.post_save_hook = script_post_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%notebook \"/Users/posnerlab/Documents/GitHub/ITA/filename.ipynb\"     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "%notebook /Users/posnerlab/Documents/GitHub/ITA/filename.ipynb     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%notebook /Users/posnerlab/Documents/GitHub/ITA/Poweratlas_Coherence_extract.ipynb     "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
